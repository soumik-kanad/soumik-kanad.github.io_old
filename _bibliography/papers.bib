
---
---


@inproceedings{10.1117/12.2548589,
author = {Aditya  Bhardwaj and Jun-Sung Park and Soumik Mukhopadhyay and Sikander Sharda and Yuri Son and Bhavya  Ajani and Srinivas Rao Kudavelly},
title = {{Rigid and deformable corrections in real-time using deep learning for prostate fusion biopsy}},
volume = {11315},
booktitle = {Medical Imaging 2020: Image-Guided Procedures, Robotic Interventions, and Modeling},
editor = {Baowei Fei and Cristian A. Linte},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {113151W},
abstract = {Fusion biopsy reduces false negative rates in prostatic cancer detection compare to systemic biopsy. However, accuracy in biopsy sampling depends upon quality of alignment between pre-operative 3D MR and intra-operative 2D US. During live biopsy, the US-MR alignment may be disturbed due to prostate or patient rigid motion. Further, prostate gland deform due to probe pressure, which add error in biopsy sampling. In this paper, we describe a method for real-time 2D-3D multimodal registration, utilizing deep learning, to correct for rigid and deformable errors. Our method do not require an intermediate 3D US and works in real-time with an average runtime of 112 ms for both rigid and deformable corrections. On 12 patient data, our method reduces mean trans-registration error (TRE) from 8.890&plusmn;5.106 mm to 2.988&plusmn;1.513 mm, comparable to other state of the arts in accuracy.},
keywords = {Fusion Biopsy, Deep Learning, Motion Correction, Multi-modal registration, Prostate Segmentation},
year = {2020},
doi = {10.1117/12.2548589},
URL = {https://doi.org/10.1117/12.2548589},
pdf = {https://drive.google.com/file/d/1kgFFl2w84688ZuOqFAK298TC_tpOACq8/view?usp=drive_link},
selected={true}
}



@inproceedings{10.1117/12.2580891,
author = {Soumik Mukhopadhyay and Praful Mathur and Aditya Bharadwaj and Yuri Son and Jun-Sung Park and Srinivas Rao Kudavelly and Sangha Song and Hokyung Kang},
title = {{Deep learning based needle tracking in prostate fusion biopsy}},
volume = {11598},
booktitle = {Medical Imaging 2021: Image-Guided Procedures, Robotic Interventions, and Modeling},
editor = {Cristian A. Linte and Jeffrey H. Siewerdsen},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {115982A},
abstract = {Fusion of pre-operative Magnetic Resonance Imaging (MRI) and Trans-Rectal Ultrasound (TRUS) guided biopsy (Fusion Biopsy) has proven to be more effective as compared to cognitive biopsy for the detection of prostate cancer. The detection of the biopsy needle used during the Ultrasound procedure has multiple applications like reporting, repeat biopsy planning and planning therapy. Earlier methods to solve this problem have only used image processing techniques like Hough- Transform or Graph-Cut. These techniques lack robustness because only image-based solution cannot take care of the huge variability in the data as well as the problem of needle going out of plane. Recent deep learning (DL) based solutions for needle detection have high latency and does not exploit temporal information present in TRUS imaging. In this paper, we propose a method to automatically detect the short-lived needle triggers and its position using temporal context incorporated into a DL model termed as Samsung Multi-Decoder Network (S-MDNet). The proposed solution has been tested on 8 patients and yields high sensitivity (96%) and specificity (95%) for the detection of the needle trigger event.},
keywords = {Prostate Fusion Biopsy, Deep Learning, Needle Detection, Biopsy Localization},
year = {2021},
doi = {10.1117/12.2580891},
URL = {https://doi.org/10.1117/12.2580891},
pdf = {https://drive.google.com/file/d/15mceqcOsFydYSP1QNSu1zoSiTNydfRh_/view?usp=drive_link},
selected={true}
}
